{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hand Gesture Recognizer to control a robot with ROS This documentation aims to help users and developers to use or extend the hand gesture recognizer developed for the Master's thesis \"Dynamic Human Robot Interaction Framework Using Deep Learning and Robot Operating System (ROS): a practical approach\". This work is available on GitHub . Outline Use the program Hand gesture recognizer Controller","title":"Home"},{"location":"#hand-gesture-recognizer-to-control-a-robot-with-ros","text":"This documentation aims to help users and developers to use or extend the hand gesture recognizer developed for the Master's thesis \"Dynamic Human Robot Interaction Framework Using Deep Learning and Robot Operating System (ROS): a practical approach\". This work is available on GitHub .","title":"Hand Gesture Recognizer to control a robot with ROS"},{"location":"#outline","text":"Use the program Hand gesture recognizer Controller","title":"Outline"},{"location":"macro/","text":"Macro Macros are sequence of commands pre-recorded. They are saved as text file in which each line represents a gesture done by the operator. You can create them writing the text file, or using the program while doing hand gestures. Create a new macro Using the program Launch the script with the argument --interactive : python3 main.py --interactive Choose the option 2 - Macro mode (write 2 and press enter ) Choose the option 0 - Create a new macro Write the name for the new macro and press enter Do the sequence of gesture you want to record. When you are done press esc Writing the text file Create a text file and write the name of the gestures you want to save. One per line. The name of the gestures must be the same as those written in the automaton configuration file. The following example show a macro that: Pick up the parcel A Go to the position B Drop down the parcel pick_up a go_to b drop_down Run a macro Launch the script with the argument --interactive : python3 main.py --interactive Choose the option 2 - Macro mode (write 2 and press enter ) Choose the option 1 - Run an existent one Write the path of the text file and press enter","title":"Macro"},{"location":"macro/#macro","text":"Macros are sequence of commands pre-recorded. They are saved as text file in which each line represents a gesture done by the operator. You can create them writing the text file, or using the program while doing hand gestures.","title":"Macro"},{"location":"macro/#create-a-new-macro","text":"","title":"Create a new macro"},{"location":"macro/#using-the-program","text":"Launch the script with the argument --interactive : python3 main.py --interactive Choose the option 2 - Macro mode (write 2 and press enter ) Choose the option 0 - Create a new macro Write the name for the new macro and press enter Do the sequence of gesture you want to record. When you are done press esc","title":"Using the program"},{"location":"macro/#writing-the-text-file","text":"Create a text file and write the name of the gestures you want to save. One per line. The name of the gestures must be the same as those written in the automaton configuration file. The following example show a macro that: Pick up the parcel A Go to the position B Drop down the parcel pick_up a go_to b drop_down","title":"Writing the text file"},{"location":"macro/#run-a-macro","text":"Launch the script with the argument --interactive : python3 main.py --interactive Choose the option 2 - Macro mode (write 2 and press enter ) Choose the option 1 - Run an existent one Write the path of the text file and press enter","title":"Run a macro"},{"location":"run/","text":"Run the simulation After the setup of the environment you can run the simulation how many times you want. Launch the Gazebo world Empty world ros2 launch turtlebot3_gazebo empty_world.launch.py AWS small warehouse ros2 launch turtlebot3_gazebo turtlebot3_house.launch.py Launch the Nav2 system ros2 launch turtlebot3_navigation2 navigation2.launch.py use_sim_time: = True map: = PATH_TO_THE_MAP Map of the environment The map of the warehouse is available on the repository of this project. You can create the map for your environments. Launch the hand gesture recognizer Inside the directory of the repository python3 main.py --interactive The --interactive argument start the program asking to the user how they want to use it. If you prefer to use the operational mode remove the argument.","title":"Run the simulation"},{"location":"run/#run-the-simulation","text":"After the setup of the environment you can run the simulation how many times you want.","title":"Run the simulation"},{"location":"run/#launch-the-gazebo-world","text":"","title":"Launch the Gazebo world"},{"location":"run/#empty-world","text":"ros2 launch turtlebot3_gazebo empty_world.launch.py","title":"Empty world"},{"location":"run/#aws-small-warehouse","text":"ros2 launch turtlebot3_gazebo turtlebot3_house.launch.py","title":"AWS small warehouse"},{"location":"run/#launch-the-nav2-system","text":"ros2 launch turtlebot3_navigation2 navigation2.launch.py use_sim_time: = True map: = PATH_TO_THE_MAP Map of the environment The map of the warehouse is available on the repository of this project. You can create the map for your environments.","title":"Launch the Nav2 system"},{"location":"run/#launch-the-hand-gesture-recognizer","text":"Inside the directory of the repository python3 main.py --interactive The --interactive argument start the program asking to the user how they want to use it. If you prefer to use the operational mode remove the argument.","title":"Launch the hand gesture recognizer"},{"location":"setup/","text":"Setup and configuration Requirements Python >= 3.8 with pip ROS Galactic Geochelone (to test the system with a robot) NAV2 Gazebo (to test the system with a robot) TurtleBot3 Gazebo plugin Setup the environment Clone the repository and install the dependencies Clone the repository git clone git@github.com:jjocram/hand-gesture.git Install the requirements (you can use a virtual-environment) pip install -r requirements.txt Prepare the simulation environment TurtleBot3 Follow the instruction to install the package available on their website . In particular export the robot model to use it in Gazebo export GAZEBO_MODEL_PATH = \"PATH_TO_TURTLEBOT3_WORKSPACE/src/turtlebot3_simulations/turtlebot3_gazebo/models\" and the robot model export TURTLEBOT3_MODEL = waffle_pi Gazebo world You can choose between an empty world and a pre-built warehouse world. Nothing stops you from creating your world. AWS small warehouse This is the one mainly used for testing. It is a complex environment developed by amazon Clone the repository git clone https://github.com/aws-robotics/aws-robomaker-small-warehouse-world.git Export the model export GAZEBO_MODEL_PATH = \"PATH_TO_THE_AWS_REPOSITORY/models: $GAZEBO_MODEL_PATH \" Check the variable GAZEBO_MODEL_PATH Check that the environment variable GAZEBO_MODEL_PATH has both the path of the TurtleBot3 model and AWS warehouse Add the Waffle Pi in the AWS small warehouse world Edit the file PATH_TO_THE_AWS_REPOSITORY/worlds/no_roof_small_warehouse.world adding <include> <uri> model://turtlebot3_waffle_pi </uri> </include> After last <model>...</model> and before <light>...</light> Edit the launch file Edit the file PATH_TO_TURTLEBOT3_WORKSPACE/src/turtlebot3_simulations/turtlebot3_gazebo/launch/turtlebot3_house.launch.py replacing world_file_name = 'turtlebot3_houses/' + TURTLEBOT3_MODEL + '.model' world = os . path . join ( get_package_share_directory ( 'turtlebot3_gazebo' ), 'worlds' , world_file_name ) with world = \"PATH_TO_THE_AWS_REPOSITORY/worlds/no_roof_small_warehouse.world\"","title":"Setup"},{"location":"setup/#setup-and-configuration","text":"","title":"Setup and configuration"},{"location":"setup/#requirements","text":"Python >= 3.8 with pip ROS Galactic Geochelone (to test the system with a robot) NAV2 Gazebo (to test the system with a robot) TurtleBot3 Gazebo plugin","title":"Requirements"},{"location":"setup/#setup-the-environment","text":"Clone the repository and install the dependencies","title":"Setup the environment"},{"location":"setup/#clone-the-repository","text":"git clone git@github.com:jjocram/hand-gesture.git","title":"Clone the repository"},{"location":"setup/#install-the-requirements-you-can-use-a-virtual-environment","text":"pip install -r requirements.txt","title":"Install the requirements (you can use a virtual-environment)"},{"location":"setup/#prepare-the-simulation-environment","text":"","title":"Prepare the simulation environment"},{"location":"setup/#turtlebot3","text":"Follow the instruction to install the package available on their website . In particular export the robot model to use it in Gazebo export GAZEBO_MODEL_PATH = \"PATH_TO_TURTLEBOT3_WORKSPACE/src/turtlebot3_simulations/turtlebot3_gazebo/models\" and the robot model export TURTLEBOT3_MODEL = waffle_pi","title":"TurtleBot3"},{"location":"setup/#gazebo-world","text":"You can choose between an empty world and a pre-built warehouse world. Nothing stops you from creating your world.","title":"Gazebo world"},{"location":"setup/#aws-small-warehouse","text":"This is the one mainly used for testing. It is a complex environment developed by amazon","title":"AWS small warehouse"},{"location":"setup/#clone-the-repository_1","text":"git clone https://github.com/aws-robotics/aws-robomaker-small-warehouse-world.git","title":"Clone the repository"},{"location":"setup/#export-the-model","text":"export GAZEBO_MODEL_PATH = \"PATH_TO_THE_AWS_REPOSITORY/models: $GAZEBO_MODEL_PATH \" Check the variable GAZEBO_MODEL_PATH Check that the environment variable GAZEBO_MODEL_PATH has both the path of the TurtleBot3 model and AWS warehouse","title":"Export the model"},{"location":"setup/#add-the-waffle-pi-in-the-aws-small-warehouse-world","text":"Edit the file PATH_TO_THE_AWS_REPOSITORY/worlds/no_roof_small_warehouse.world adding <include> <uri> model://turtlebot3_waffle_pi </uri> </include> After last <model>...</model> and before <light>...</light>","title":"Add the Waffle Pi in the AWS small warehouse world"},{"location":"setup/#edit-the-launch-file","text":"Edit the file PATH_TO_TURTLEBOT3_WORKSPACE/src/turtlebot3_simulations/turtlebot3_gazebo/launch/turtlebot3_house.launch.py replacing world_file_name = 'turtlebot3_houses/' + TURTLEBOT3_MODEL + '.model' world = os . path . join ( get_package_share_directory ( 'turtlebot3_gazebo' ), 'worlds' , world_file_name ) with world = \"PATH_TO_THE_AWS_REPOSITORY/worlds/no_roof_small_warehouse.world\"","title":"Edit the launch file"},{"location":"train/","text":"Train the networks Use the train package to create new tflite files The train package has been implemented to let to everyone train their own networks based on their own dataset. You can launch the train with the command python3 -m train <static_gesture | dynamic_gesture> --train-name TRAIN_NAME [ --sample-number N ] With static_gesture you will train on the dataset of the static gestures. With dynamic_gesture you will train on the dataset of the dynamic gesture. --train-name TRAIN_NAME set the name of the train as TRAIN_NAME . It will be used as name for TensorBoard. --sample-number N is an optional argument. If it is passed, the size of the dataset will be N * num_classes Create a new gesture or increase the samples in the dataset of an existing one Launch the application in iteractive mode: python3 main.py --interactive Choose 1 - Learing mode : write 1 and press enter Choose on which dataset you want to operate: 0 for static, 1 for dynamic Choose if you want create a new gesture ( 0 ) or enhance an already existent one ( 1 ) If you choose to create a new gesture then, you will be asked for the name of the gesture If you choose to enhance and already existent one then, you will be asked for the id of the gesture to enhance The program will start and you can press the space bar to write on the dataset a snapshot of the position of your hand","title":"Train"},{"location":"train/#train-the-networks","text":"","title":"Train the networks"},{"location":"train/#use-the-train-package-to-create-new-tflite-files","text":"The train package has been implemented to let to everyone train their own networks based on their own dataset. You can launch the train with the command python3 -m train <static_gesture | dynamic_gesture> --train-name TRAIN_NAME [ --sample-number N ] With static_gesture you will train on the dataset of the static gestures. With dynamic_gesture you will train on the dataset of the dynamic gesture. --train-name TRAIN_NAME set the name of the train as TRAIN_NAME . It will be used as name for TensorBoard. --sample-number N is an optional argument. If it is passed, the size of the dataset will be N * num_classes","title":"Use the train package to create new tflite files"},{"location":"train/#create-a-new-gesture-or-increase-the-samples-in-the-dataset-of-an-existing-one","text":"Launch the application in iteractive mode: python3 main.py --interactive Choose 1 - Learing mode : write 1 and press enter Choose on which dataset you want to operate: 0 for static, 1 for dynamic Choose if you want create a new gesture ( 0 ) or enhance an already existent one ( 1 ) If you choose to create a new gesture then, you will be asked for the name of the gesture If you choose to enhance and already existent one then, you will be asked for the id of the gesture to enhance The program will start and you can press the space bar to write on the dataset a snapshot of the position of your hand","title":"Create a new gesture or increase the samples in the dataset of an existing one"},{"location":"development/automata_example/","text":"Example of configuration file { \"initial_state\" : \"q0\" , \"transitions\" : [ { \"from\" : \"q0\" , \"to\" : \"q1\" , \"with\" : \"pick_up\" , \"action\" : null }, { \"from\" : \"q0\" , \"to\" : \"q2\" , \"with\" : \"go_to\" , \"action\" : null }, { \"from\" : \"q2\" , \"to\" : \"q0\" , \"with\" : \"A-Z\" , \"action\" : { \"type\" : \"set_navigation_goal\" , \"coordinate\" : \"$with\" } }, { \"from\" : \"q1\" , \"to\" : \"q3\" , \"with\" : \"A-Z\" , \"action\" : { \"type\" : \"send_message\" , \"message\" : { \"type\" : \"std_msgs.msg.String\" , \"topic\" : \"/control_arm\" , \"fields\" : { \"data\" : \"pick_up $with\" } } } }, { \"from\" : \"q3\" , \"to\" : \"q0\" , \"with\" : \"drop_down\" , \"action\" : { \"type\" : \"send_message\" , \"message\" : { \"type\" : \"std_msgs.msg.String\" , \"topic\" : \"/control_arm\" , \"fields\" : { \"data\" : \"drop_down\" } } } }, { \"from\" : \"q3\" , \"to\" : \"q4\" , \"with\" : \"go_to\" , \"action\" : null }, { \"from\" : \"q4\" , \"to\" : \"q3\" , \"with\" : \"A-Z\" , \"action\" : { \"type\" : \"set_navigation_goal\" , \"coordinate\" : \"$with\" } } ] } Automaton generated with the above configuration files","title":"Example"},{"location":"development/automata_example/#example-of-configuration-file","text":"{ \"initial_state\" : \"q0\" , \"transitions\" : [ { \"from\" : \"q0\" , \"to\" : \"q1\" , \"with\" : \"pick_up\" , \"action\" : null }, { \"from\" : \"q0\" , \"to\" : \"q2\" , \"with\" : \"go_to\" , \"action\" : null }, { \"from\" : \"q2\" , \"to\" : \"q0\" , \"with\" : \"A-Z\" , \"action\" : { \"type\" : \"set_navigation_goal\" , \"coordinate\" : \"$with\" } }, { \"from\" : \"q1\" , \"to\" : \"q3\" , \"with\" : \"A-Z\" , \"action\" : { \"type\" : \"send_message\" , \"message\" : { \"type\" : \"std_msgs.msg.String\" , \"topic\" : \"/control_arm\" , \"fields\" : { \"data\" : \"pick_up $with\" } } } }, { \"from\" : \"q3\" , \"to\" : \"q0\" , \"with\" : \"drop_down\" , \"action\" : { \"type\" : \"send_message\" , \"message\" : { \"type\" : \"std_msgs.msg.String\" , \"topic\" : \"/control_arm\" , \"fields\" : { \"data\" : \"drop_down\" } } } }, { \"from\" : \"q3\" , \"to\" : \"q4\" , \"with\" : \"go_to\" , \"action\" : null }, { \"from\" : \"q4\" , \"to\" : \"q3\" , \"with\" : \"A-Z\" , \"action\" : { \"type\" : \"set_navigation_goal\" , \"coordinate\" : \"$with\" } } ] } Automaton generated with the above configuration files","title":"Example of configuration file"},{"location":"development/automata_manager/","text":"Automata manager The class AutomataManager handles the logic of the automaton and uses ROS to communicate with the robot. Initialization def __init__ ( self , path : str , execute_actions : bool ): global _initialized # (1) self . execute_actions = execute_actions and ROS_AVAILABLE # (2) with open ( path ) as json_file : automata_dict = load ( json_file ) self . current_state = automata_dict . get ( \"initial_state\" ) self . transitions = [ Transition ( t . get ( \"from\" ), t . get ( \"to\" ), t . get ( \"with\" ), t . get ( \"action\" )) for t in automata_dict . get ( \"transitions\" )] self . states = { extract ( state ) for state in self . transitions for extract in ( lambda transition : transition . from_state , lambda transition : transition . to_state )} self . alphabet = { transition . with_what for transition in self . transitions } - self . special_chars self . message_publisher = {( t . action . get ( \"message\" ) . get ( \"type\" ), t . action . get ( \"message\" ) . get ( \"topic\" )): None for t in self . transitions if t . action and t . action . get ( \"type\" ) == \"send_message\" } if self . execute_actions : if not _initialized : rclpy . init () self . _node = Node ( \"hand_gesture_recognizer\" ) for message_type , message_topic in self . message_publisher : pkg = \".\" . join ( message_type . split ( \".\" )[: - 1 ]) type_to_import = message_type . split ( \".\" )[ - 1 ] if type_to_import not in globals (): globals () . update ({ type_to_import : getattr ( import_module ( pkg ), type_to_import )}) self . message_publisher [( message_type , message_topic )] = self . _node . create_publisher ( globals ()[ type_to_import ], message_topic , 10 ) print ( self . message_publisher ) self . navigator = BasicNavigator () self . initial_pose = PoseStamped () self . initial_pose . header . frame_id = 'map' self . initial_pose . header . stamp = self . navigator . get_clock () . now () . to_msg () self . initial_pose . pose . position . x = 0.0 self . initial_pose . pose . position . y = 0.0 self . initial_pose . pose . orientation . z = 0.70 self . initial_pose . pose . orientation . w = 0.71 if not _initialized : self . navigator . setInitialPose ( self . initial_pose ) self . navigator . waitUntilNav2Active () else : self . navigator = None _initialized = True The global variable _initialized is used to avoid a double initialization of rclpy execute_actions is True if the user wants it and ROS is available on the system In the __init__ function the automaton is initialized. First of all the configuration file is read and temporarily saved into a dict . Then automaton information are extracted from it: - current_state : initially it is the initial_state - transitions : the list of transitions saved into NamedTuple s - states : the set of states is extracted from the list of transitions - alphabet : the automaton's alphabet is extracted from the transitions and then the set of special elements is removed from it - message_publisher : dictionary that map the type of a message and the topic with a ROS' publisher (it will initialized later) Then, if the automaton has to execute the actions, the init function will create the correct environment: It will call rclpy . init () exactly once It will create the Node that will communicate with others It will parse from the transitions and import the packages required to send message_publisher It will create the message publisher one for each pair of (type, topic) It will initialize the navigator It will create the initial position of the robot It will set the initial position and will wait until the navigator will be ready Communicate with the robot def _navigate_to ( self , position ): if self . execute_actions : self . navigator . goToPose ( position ) i = 0 while not self . navigator . isTaskComplete (): i += 1 feedback = self . navigator . getFeedback () if feedback and i % 10 == 0 : print ( f \"Robot is going to position...\" ) result = self . navigator . getResult () if result == TaskResult . SUCCEEDED : print ( 'Goal succeeded!' ) elif result == TaskResult . CANCELED : raise Exception ( 'Goal was canceled!' ) elif result == TaskResult . FAILED : raise Exception ( 'Goal failed!' ) else : print ( \"Navigation to\" , position ) def _send_message ( self , publisher_identifier , message_fields ): print ( publisher_identifier ) print ( self . message_publisher ) if self . execute_actions : message_type = publisher_identifier [ 0 ] . split ( \".\" )[ - 1 ] # (1) message = globals ()[ message_type ]() # (2) for mf in message_fields : setattr ( message , mf , message_fields [ mf ]) self . message_publisher [ publisher_identifier ] . publish ( message ) else : print ( f \"Publishing message: { publisher_identifier } , { message_fields } \" ) Extraction of the type of the message to send Creation of the object of the correct type Navigate to a position The function _navigate_to(self, position) will handle the navigation to a given PoseStamped . First of all the goToPose of the object self.navigator is called and then, until the task is complete it will receive a feedback from the navigator. The Task can end in 3 ways: Succeeded: everything went fine and the robot reached its goal Canceled: the user has canceled the task Failed: the robot couldn't reach its goal Send a message The function _send_message(self, publisher_identifier, message_fields) will handle the sending of messages. The message type is extracted from the publisher_identifier and then it is used to create a new object with that type message = globals ()[ message_type ]() . Then the method setattr is used to add the fields necessary for the message. Finally the right publisher is used to send the message. Consume input Them main purpose of the automaton is consuming the user input and reacting to certain inputs. The function consume_input(self, specific_input) handle this purpose. def consume_input ( self , specific_input ) -> bool : input_accepted = True # Get all transition from the current state transitions_from_current_state = [ transition for transition in self . transitions if transition . from_state == self . current_state ] # Get the \"generic input\" from the one given (i.e. a letter becomes \"A-Z\") generic_input = self . _get_generic_input ( specific_input ) # Get the transaction that match the given input try : transition = next ( filter ( lambda tr : tr . with_what == generic_input , transitions_from_current_state )) except StopIteration : # print(f\"Transition not found from {self.current_state} with {generic_input}\") # Input not accepted for the current state return False # Update current automata state self . current_state = transition . to_state # Execute command TODO: from python3.10 use a match-case statement if transition . action is None : print ( \"No action required\" ) elif transition . action . get ( \"type\" ) == \"set_navigation_goal\" : raw_position = transition . action . get ( \"coordinate\" ) if raw_position == '$with' : if specific_input in WAREHOUSE_MAP . keys (): position = self . _get_pose_stamped ( WAREHOUSE_MAP . get ( specific_input )) else : print ( f \"Position ( { specific_input } ) not in database\" ) position = None else : x , y = raw_position . split () position = self . _get_pose_stamped (( x , y )) if position : self . _navigate_to ( position ) elif transition . action . get ( \"type\" ) == \"send_message\" : message = transition . action . get ( \"message\" ) message_type = message . get ( \"type\" ) message_topic = message . get ( \"topic\" ) message_fields = message . get ( \"fields\" ) for mf in message_fields : message_fields [ mf ] = message_fields [ mf ] . replace ( \"$with\" , specific_input ) self . _send_message ( publisher_identifier = ( message_type , message_topic ), message_fields = message_fields ) else : print ( \"Action not supported\" ) return input_accepted First of all it identify the correct transition. If none is found then it returns False . Otherwise it checks its action type: No action: If no action is to be performed then the function will simply return input_accepted Set navigation goal In this case the coordinate field is check for the $with . If it there is, then, the specific user input is used (e.g. the letter of the alphabet to identify a position). Otherwise the coordinates are taken from the coordinate field of the action. Send a message In this case the different component necessary to create and send the message are extracted from the action and every $with found inside a field is replaced with the specific input that triggered the transition. Utility function Get PoseStamped def _get_pose_stamped ( self , position ): if self . execute_actions : goal_pose = PoseStamped () goal_pose . header . frame_id = 'map' goal_pose . header . stamp = self . navigator . get_clock () . now () . to_msg () goal_pose . pose . position . x = position [ 0 ] goal_pose . pose . position . y = position [ 1 ] return goal_pose return position Create an object of type PoseStamped given a pair of coordinates as input. Get generic input def _get_generic_input ( self , specific_input ): if specific_input not in self . alphabet : return \"A-Z\" else : return specific_input Given the user input it will check if it is a special input that must be parsed.","title":"AutomataManager"},{"location":"development/automata_manager/#automata-manager","text":"The class AutomataManager handles the logic of the automaton and uses ROS to communicate with the robot.","title":"Automata manager"},{"location":"development/automata_manager/#initialization","text":"def __init__ ( self , path : str , execute_actions : bool ): global _initialized # (1) self . execute_actions = execute_actions and ROS_AVAILABLE # (2) with open ( path ) as json_file : automata_dict = load ( json_file ) self . current_state = automata_dict . get ( \"initial_state\" ) self . transitions = [ Transition ( t . get ( \"from\" ), t . get ( \"to\" ), t . get ( \"with\" ), t . get ( \"action\" )) for t in automata_dict . get ( \"transitions\" )] self . states = { extract ( state ) for state in self . transitions for extract in ( lambda transition : transition . from_state , lambda transition : transition . to_state )} self . alphabet = { transition . with_what for transition in self . transitions } - self . special_chars self . message_publisher = {( t . action . get ( \"message\" ) . get ( \"type\" ), t . action . get ( \"message\" ) . get ( \"topic\" )): None for t in self . transitions if t . action and t . action . get ( \"type\" ) == \"send_message\" } if self . execute_actions : if not _initialized : rclpy . init () self . _node = Node ( \"hand_gesture_recognizer\" ) for message_type , message_topic in self . message_publisher : pkg = \".\" . join ( message_type . split ( \".\" )[: - 1 ]) type_to_import = message_type . split ( \".\" )[ - 1 ] if type_to_import not in globals (): globals () . update ({ type_to_import : getattr ( import_module ( pkg ), type_to_import )}) self . message_publisher [( message_type , message_topic )] = self . _node . create_publisher ( globals ()[ type_to_import ], message_topic , 10 ) print ( self . message_publisher ) self . navigator = BasicNavigator () self . initial_pose = PoseStamped () self . initial_pose . header . frame_id = 'map' self . initial_pose . header . stamp = self . navigator . get_clock () . now () . to_msg () self . initial_pose . pose . position . x = 0.0 self . initial_pose . pose . position . y = 0.0 self . initial_pose . pose . orientation . z = 0.70 self . initial_pose . pose . orientation . w = 0.71 if not _initialized : self . navigator . setInitialPose ( self . initial_pose ) self . navigator . waitUntilNav2Active () else : self . navigator = None _initialized = True The global variable _initialized is used to avoid a double initialization of rclpy execute_actions is True if the user wants it and ROS is available on the system In the __init__ function the automaton is initialized. First of all the configuration file is read and temporarily saved into a dict . Then automaton information are extracted from it: - current_state : initially it is the initial_state - transitions : the list of transitions saved into NamedTuple s - states : the set of states is extracted from the list of transitions - alphabet : the automaton's alphabet is extracted from the transitions and then the set of special elements is removed from it - message_publisher : dictionary that map the type of a message and the topic with a ROS' publisher (it will initialized later) Then, if the automaton has to execute the actions, the init function will create the correct environment: It will call rclpy . init () exactly once It will create the Node that will communicate with others It will parse from the transitions and import the packages required to send message_publisher It will create the message publisher one for each pair of (type, topic) It will initialize the navigator It will create the initial position of the robot It will set the initial position and will wait until the navigator will be ready","title":"Initialization"},{"location":"development/automata_manager/#communicate-with-the-robot","text":"def _navigate_to ( self , position ): if self . execute_actions : self . navigator . goToPose ( position ) i = 0 while not self . navigator . isTaskComplete (): i += 1 feedback = self . navigator . getFeedback () if feedback and i % 10 == 0 : print ( f \"Robot is going to position...\" ) result = self . navigator . getResult () if result == TaskResult . SUCCEEDED : print ( 'Goal succeeded!' ) elif result == TaskResult . CANCELED : raise Exception ( 'Goal was canceled!' ) elif result == TaskResult . FAILED : raise Exception ( 'Goal failed!' ) else : print ( \"Navigation to\" , position ) def _send_message ( self , publisher_identifier , message_fields ): print ( publisher_identifier ) print ( self . message_publisher ) if self . execute_actions : message_type = publisher_identifier [ 0 ] . split ( \".\" )[ - 1 ] # (1) message = globals ()[ message_type ]() # (2) for mf in message_fields : setattr ( message , mf , message_fields [ mf ]) self . message_publisher [ publisher_identifier ] . publish ( message ) else : print ( f \"Publishing message: { publisher_identifier } , { message_fields } \" ) Extraction of the type of the message to send Creation of the object of the correct type","title":"Communicate with the robot"},{"location":"development/automata_manager/#navigate-to-a-position","text":"The function _navigate_to(self, position) will handle the navigation to a given PoseStamped . First of all the goToPose of the object self.navigator is called and then, until the task is complete it will receive a feedback from the navigator. The Task can end in 3 ways: Succeeded: everything went fine and the robot reached its goal Canceled: the user has canceled the task Failed: the robot couldn't reach its goal","title":"Navigate to a position"},{"location":"development/automata_manager/#send-a-message","text":"The function _send_message(self, publisher_identifier, message_fields) will handle the sending of messages. The message type is extracted from the publisher_identifier and then it is used to create a new object with that type message = globals ()[ message_type ]() . Then the method setattr is used to add the fields necessary for the message. Finally the right publisher is used to send the message.","title":"Send a message"},{"location":"development/automata_manager/#consume-input","text":"Them main purpose of the automaton is consuming the user input and reacting to certain inputs. The function consume_input(self, specific_input) handle this purpose. def consume_input ( self , specific_input ) -> bool : input_accepted = True # Get all transition from the current state transitions_from_current_state = [ transition for transition in self . transitions if transition . from_state == self . current_state ] # Get the \"generic input\" from the one given (i.e. a letter becomes \"A-Z\") generic_input = self . _get_generic_input ( specific_input ) # Get the transaction that match the given input try : transition = next ( filter ( lambda tr : tr . with_what == generic_input , transitions_from_current_state )) except StopIteration : # print(f\"Transition not found from {self.current_state} with {generic_input}\") # Input not accepted for the current state return False # Update current automata state self . current_state = transition . to_state # Execute command TODO: from python3.10 use a match-case statement if transition . action is None : print ( \"No action required\" ) elif transition . action . get ( \"type\" ) == \"set_navigation_goal\" : raw_position = transition . action . get ( \"coordinate\" ) if raw_position == '$with' : if specific_input in WAREHOUSE_MAP . keys (): position = self . _get_pose_stamped ( WAREHOUSE_MAP . get ( specific_input )) else : print ( f \"Position ( { specific_input } ) not in database\" ) position = None else : x , y = raw_position . split () position = self . _get_pose_stamped (( x , y )) if position : self . _navigate_to ( position ) elif transition . action . get ( \"type\" ) == \"send_message\" : message = transition . action . get ( \"message\" ) message_type = message . get ( \"type\" ) message_topic = message . get ( \"topic\" ) message_fields = message . get ( \"fields\" ) for mf in message_fields : message_fields [ mf ] = message_fields [ mf ] . replace ( \"$with\" , specific_input ) self . _send_message ( publisher_identifier = ( message_type , message_topic ), message_fields = message_fields ) else : print ( \"Action not supported\" ) return input_accepted First of all it identify the correct transition. If none is found then it returns False . Otherwise it checks its action type:","title":"Consume input"},{"location":"development/automata_manager/#no-action","text":"If no action is to be performed then the function will simply return input_accepted","title":"No action:"},{"location":"development/automata_manager/#set-navigation-goal","text":"In this case the coordinate field is check for the $with . If it there is, then, the specific user input is used (e.g. the letter of the alphabet to identify a position). Otherwise the coordinates are taken from the coordinate field of the action.","title":"Set navigation goal"},{"location":"development/automata_manager/#send-a-message_1","text":"In this case the different component necessary to create and send the message are extracted from the action and every $with found inside a field is replaced with the specific input that triggered the transition.","title":"Send a message"},{"location":"development/automata_manager/#utility-function","text":"","title":"Utility function"},{"location":"development/automata_manager/#get-posestamped","text":"def _get_pose_stamped ( self , position ): if self . execute_actions : goal_pose = PoseStamped () goal_pose . header . frame_id = 'map' goal_pose . header . stamp = self . navigator . get_clock () . now () . to_msg () goal_pose . pose . position . x = position [ 0 ] goal_pose . pose . position . y = position [ 1 ] return goal_pose return position Create an object of type PoseStamped given a pair of coordinates as input.","title":"Get PoseStamped"},{"location":"development/automata_manager/#get-generic-input","text":"def _get_generic_input ( self , specific_input ): if specific_input not in self . alphabet : return \"A-Z\" else : return specific_input Given the user input it will check if it is a special input that must be parsed.","title":"Get generic input"},{"location":"development/automata_manager_config/","text":"Automaton configuration To configure the automaton i sufficient write a JSON file and give its path to the controllers that will use it. Basic structure The basic structure of the JSON file is { \"initial_state\" : \"string\" , \"transitions\" : [ \"Transitions\" ] } Transition A transition is triggered when the automaton is in a state that accept the gesture. When a transition is triggered an action is executed. The structure of a transition is: { \"from\" : \"string\" , \"to\" : \"string\" , \"with\" : \"string\" , \"action\" : { ... } } from : the state the automaton must be in to accept the input and trigger the action to : the state the automaton will be after executing the action with : the element of the alphabet that trigger the action action : the action to perform when the transition is triggered Action - do nothing When \"action\" : null nothing is performed when the transition is triggered. Action - send a message To publish a message on a topic you can use this action: { \"type\" : \"send_message\" , \"message\" : { \"type\" : \"string\" , \"topic\" : \"string\" , \"fields\" : { ... } } } message.type : the type of the message you want to send. It must the complete import path. E.g. \"type\" : \"std_msgs.msg.String\" will import String from std_msgs.msg message.topic : the topic on which the message will be published message.fields : the data fields to use to build the message. E.g. \"data\" : \"data to send\" will be converted into message . data = \"data to send\" Action - set a navigation goal To set a Nav 2 navigation goal you can use this action: \"action\" : { \"type\" : \"set_navigation_goal\" , \"coordinate\" : \"string\" } coordinate is the position to reach Text interpolation for data You can use the keyword $with that will be substitute at run-time with the content of the with field of the transition. It is useful in particular, when the trigger is a set of elements of the alphabet. E.g. the set of the letters [Aa-Zz] .","title":"Configuration"},{"location":"development/automata_manager_config/#automaton-configuration","text":"To configure the automaton i sufficient write a JSON file and give its path to the controllers that will use it.","title":"Automaton configuration"},{"location":"development/automata_manager_config/#basic-structure","text":"The basic structure of the JSON file is { \"initial_state\" : \"string\" , \"transitions\" : [ \"Transitions\" ] }","title":"Basic structure"},{"location":"development/automata_manager_config/#transition","text":"A transition is triggered when the automaton is in a state that accept the gesture. When a transition is triggered an action is executed. The structure of a transition is: { \"from\" : \"string\" , \"to\" : \"string\" , \"with\" : \"string\" , \"action\" : { ... } } from : the state the automaton must be in to accept the input and trigger the action to : the state the automaton will be after executing the action with : the element of the alphabet that trigger the action action : the action to perform when the transition is triggered","title":"Transition"},{"location":"development/automata_manager_config/#action-do-nothing","text":"When \"action\" : null nothing is performed when the transition is triggered.","title":"Action - do nothing"},{"location":"development/automata_manager_config/#action-send-a-message","text":"To publish a message on a topic you can use this action: { \"type\" : \"send_message\" , \"message\" : { \"type\" : \"string\" , \"topic\" : \"string\" , \"fields\" : { ... } } } message.type : the type of the message you want to send. It must the complete import path. E.g. \"type\" : \"std_msgs.msg.String\" will import String from std_msgs.msg message.topic : the topic on which the message will be published message.fields : the data fields to use to build the message. E.g. \"data\" : \"data to send\" will be converted into message . data = \"data to send\"","title":"Action - send a message"},{"location":"development/automata_manager_config/#action-set-a-navigation-goal","text":"To set a Nav 2 navigation goal you can use this action: \"action\" : { \"type\" : \"set_navigation_goal\" , \"coordinate\" : \"string\" } coordinate is the position to reach","title":"Action - set a navigation goal"},{"location":"development/automata_manager_config/#text-interpolation-for-data","text":"You can use the keyword $with that will be substitute at run-time with the content of the with field of the transition. It is useful in particular, when the trigger is a set of elements of the alphabet. E.g. the set of the letters [Aa-Zz] .","title":"Text interpolation for data"},{"location":"development/base_controller/","text":"Base controller As base class there is the BaseController . It manages: The thread to avoid duplicate messages The interaction with the automaton The choice of the gesture (between static and dynamic) Get gesture This function get one gesture from the two buffers given as input. The precedence is given to the dynamic gesture. This decision has been taken because a dynamic gesture requires an action by the operator. Instead, a static one is always recognized. def _get_gesture ( self , static_gesture_buffer : GestureBuffer , dynamic_gesture_buffer : GestureBuffer ): static_gesture = None static_gesture_id = static_gesture_buffer . get_gesture () if static_gesture_id is not None and static_gesture_id != - 1 : if self . last_static != static_gesture_id : # print(\"Static gesture id:\", static_gesture_id) self . last_static = static_gesture_id if self . last_static != - 1 : static_gesture = self . static_gesture_map . get ( self . last_static , None ) dynamic_gesture = None dynamic_gesture_id = dynamic_gesture_buffer . get_gesture () if dynamic_gesture_id is not None and dynamic_gesture_buffer != - 1 : if self . last_dynamic != dynamic_gesture_id : # print(\"Dynamic gesture id:\", dynamic_gesture_id) self . last_dynamic = dynamic_gesture_id dynamic_gesture = self . dynamic_gesture_map . get ( self . last_dynamic , None ) return dynamic_gesture if dynamic_gesture else static_gesture Consume gesture The method _consume_gesture manage the access to the automaton using a Lock . In this way only one gesture at a time can be given as input to the automaton. Moreover, it ignore the static dynamic gesture. Finally, if the gesture is accepted by the automaton it calls a callback and wait for 4 seconds before releasing the Lock . def _consume_gesture ( self , static_gesture_buffer : GestureBuffer , dynamic_gesture_buffer : GestureBuffer , input_accepted_callback ): with self . sending_message : input_gesture = self . _get_gesture ( static_gesture_buffer , dynamic_gesture_buffer ) if input_gesture and input_gesture != \"static\" : input_accepted = self . automata . consume_input ( input_gesture ) if input_accepted : if input_accepted_callback : input_accepted_callback ( input_gesture ) sleep ( 4 ) The consume_gesture method must be implemented by the sub-classes. def consume_gesture ( self , static_gesture_buffer : GestureBuffer , dynamic_gesture_buffer : GestureBuffer ): raise NotImplementedError","title":"BaseController"},{"location":"development/base_controller/#base-controller","text":"As base class there is the BaseController . It manages: The thread to avoid duplicate messages The interaction with the automaton The choice of the gesture (between static and dynamic)","title":"Base controller"},{"location":"development/base_controller/#get-gesture","text":"This function get one gesture from the two buffers given as input. The precedence is given to the dynamic gesture. This decision has been taken because a dynamic gesture requires an action by the operator. Instead, a static one is always recognized. def _get_gesture ( self , static_gesture_buffer : GestureBuffer , dynamic_gesture_buffer : GestureBuffer ): static_gesture = None static_gesture_id = static_gesture_buffer . get_gesture () if static_gesture_id is not None and static_gesture_id != - 1 : if self . last_static != static_gesture_id : # print(\"Static gesture id:\", static_gesture_id) self . last_static = static_gesture_id if self . last_static != - 1 : static_gesture = self . static_gesture_map . get ( self . last_static , None ) dynamic_gesture = None dynamic_gesture_id = dynamic_gesture_buffer . get_gesture () if dynamic_gesture_id is not None and dynamic_gesture_buffer != - 1 : if self . last_dynamic != dynamic_gesture_id : # print(\"Dynamic gesture id:\", dynamic_gesture_id) self . last_dynamic = dynamic_gesture_id dynamic_gesture = self . dynamic_gesture_map . get ( self . last_dynamic , None ) return dynamic_gesture if dynamic_gesture else static_gesture","title":"Get gesture"},{"location":"development/base_controller/#consume-gesture","text":"The method _consume_gesture manage the access to the automaton using a Lock . In this way only one gesture at a time can be given as input to the automaton. Moreover, it ignore the static dynamic gesture. Finally, if the gesture is accepted by the automaton it calls a callback and wait for 4 seconds before releasing the Lock . def _consume_gesture ( self , static_gesture_buffer : GestureBuffer , dynamic_gesture_buffer : GestureBuffer , input_accepted_callback ): with self . sending_message : input_gesture = self . _get_gesture ( static_gesture_buffer , dynamic_gesture_buffer ) if input_gesture and input_gesture != \"static\" : input_accepted = self . automata . consume_input ( input_gesture ) if input_accepted : if input_accepted_callback : input_accepted_callback ( input_gesture ) sleep ( 4 ) The consume_gesture method must be implemented by the sub-classes. def consume_gesture ( self , static_gesture_buffer : GestureBuffer , dynamic_gesture_buffer : GestureBuffer ): raise NotImplementedError","title":"Consume gesture"},{"location":"development/controllers/","text":"Controller To control the action to take when a gesture is received as input a class hierarchy has been developed","title":"Controller"},{"location":"development/controllers/#controller","text":"To control the action to take when a gesture is received as input a class hierarchy has been developed","title":"Controller"},{"location":"development/development_intro/","text":"Development If you want to contribute to the project or understand how does it work you can start from here. Main The starting point of the program is the main.py file. In this file the initial interaction with the user is handled. class Mode ( Enum ): OPERATIONAL = 0 EDIT_STATIC_GESTURE = 1 EDIT_DYNAMIC_GESTURE = 2 CREATE_NEW_MACRO = auto () RUN_MACRO = auto () The enum Mode is used to to keep track of the mode selected by the user. OPERATIONAL , EDIT_STATIC_GESTURE , and EDIT_DYNAMIC_GESTURE are fixed to 0, 1, 2 for retro-compatibility. Interactive mode When the user pass the argument --interactive the program will ask some questions to the user. The methods that handle this are: - get_mode_and_number_with_interaction - learning_mode_get_mode_and_number - macro_mode Main function Configurations First of all, the main function initialized some variables: Files model_keypoint_classifier_path : path to the .tflite file for the static hand gesture recognizer labels_keypoint_classifier_path : path to the .csv file with the labels of the static hand gesture recognizer model_point_history_classifier_path : path to the .tflite file for the dynamic hand gesture recognizer labels_point_history_classifier_path : path to the .csv file with the labels of the dynamic hand gesture recognizer automata_descriptor_path : path to the .json file used to describe the automaton OpenCV configuration cap_device : id of the capture device cap_width cap_height MediaPipe configuration min_detection_confidence min_tracking_confidence Then the GestureController and the MacroController are created. Run macro If the mode is RUN_MACRO then, the run method of the MacroRunner is invoked. if mode is Mode . RUN_MACRO : MacroRunner ( options , automata_descriptor_path ) . run () Hand gesture recognizer Otherwise while ESC is not pressed the webcam is used to recognize both static and dynamic hand gesture. They are appended to two global buffers: debug_image , static_hand_gesture_id , dynamic_hand_gesture_id = gesture_detector . recognize ( image , number , mode . value , fps ) static_gesture_buffer . add_gesture ( static_hand_gesture_id ) dynamic_gesture_buffer . add_gesture ( dynamic_hand_gesture_id ) Then based on the selected mode the gesture_controller or the macro_controller are executed on a different thread (this has been made to avoid stuttering on the webcam view) if mode == Mode . OPERATIONAL : threading . Thread ( target = control , args = ( gesture_controller ,)) . start () elif mode == Mode . CREATE_NEW_MACRO : threading . Thread ( target = control , args = ( macro_controller ,)) . start ()","title":"Main"},{"location":"development/development_intro/#development","text":"If you want to contribute to the project or understand how does it work you can start from here.","title":"Development"},{"location":"development/development_intro/#main","text":"The starting point of the program is the main.py file. In this file the initial interaction with the user is handled. class Mode ( Enum ): OPERATIONAL = 0 EDIT_STATIC_GESTURE = 1 EDIT_DYNAMIC_GESTURE = 2 CREATE_NEW_MACRO = auto () RUN_MACRO = auto () The enum Mode is used to to keep track of the mode selected by the user. OPERATIONAL , EDIT_STATIC_GESTURE , and EDIT_DYNAMIC_GESTURE are fixed to 0, 1, 2 for retro-compatibility.","title":"Main"},{"location":"development/development_intro/#interactive-mode","text":"When the user pass the argument --interactive the program will ask some questions to the user. The methods that handle this are: - get_mode_and_number_with_interaction - learning_mode_get_mode_and_number - macro_mode","title":"Interactive mode"},{"location":"development/development_intro/#main-function","text":"","title":"Main function"},{"location":"development/development_intro/#configurations","text":"First of all, the main function initialized some variables: Files model_keypoint_classifier_path : path to the .tflite file for the static hand gesture recognizer labels_keypoint_classifier_path : path to the .csv file with the labels of the static hand gesture recognizer model_point_history_classifier_path : path to the .tflite file for the dynamic hand gesture recognizer labels_point_history_classifier_path : path to the .csv file with the labels of the dynamic hand gesture recognizer automata_descriptor_path : path to the .json file used to describe the automaton OpenCV configuration cap_device : id of the capture device cap_width cap_height MediaPipe configuration min_detection_confidence min_tracking_confidence Then the GestureController and the MacroController are created.","title":"Configurations"},{"location":"development/development_intro/#run-macro","text":"If the mode is RUN_MACRO then, the run method of the MacroRunner is invoked. if mode is Mode . RUN_MACRO : MacroRunner ( options , automata_descriptor_path ) . run ()","title":"Run macro"},{"location":"development/development_intro/#hand-gesture-recognizer","text":"Otherwise while ESC is not pressed the webcam is used to recognize both static and dynamic hand gesture. They are appended to two global buffers: debug_image , static_hand_gesture_id , dynamic_hand_gesture_id = gesture_detector . recognize ( image , number , mode . value , fps ) static_gesture_buffer . add_gesture ( static_hand_gesture_id ) dynamic_gesture_buffer . add_gesture ( dynamic_hand_gesture_id ) Then based on the selected mode the gesture_controller or the macro_controller are executed on a different thread (this has been made to avoid stuttering on the webcam view) if mode == Mode . OPERATIONAL : threading . Thread ( target = control , args = ( gesture_controller ,)) . start () elif mode == Mode . CREATE_NEW_MACRO : threading . Thread ( target = control , args = ( macro_controller ,)) . start ()","title":"Hand gesture recognizer"},{"location":"development/gesture_controller/","text":"Gesture controller When the program is in the operational mode this controller is used to communicate to the robot. class GestureController ( BaseController ): def __init__ ( self , keypoint_labels_path : str , point_history_labels_path : str , automata_descriptor_path : str , execute_actions : bool ): super () . __init__ ( keypoint_labels_path , point_history_labels_path , automata_descriptor_path , execute_actions ) def consume_gesture ( self , static_gesture_buffer : GestureBuffer , dynamic_gesture_buffer : GestureBuffer ): super ( GestureController , self ) . _consume_gesture ( static_gesture_buffer , dynamic_gesture_buffer , input_accepted_callback = None ) It is the easiest implementation of the BaseController . The consume_gesture method invokes the _consume_gesture from the super class without any callback.","title":"GestureController"},{"location":"development/gesture_controller/#gesture-controller","text":"When the program is in the operational mode this controller is used to communicate to the robot. class GestureController ( BaseController ): def __init__ ( self , keypoint_labels_path : str , point_history_labels_path : str , automata_descriptor_path : str , execute_actions : bool ): super () . __init__ ( keypoint_labels_path , point_history_labels_path , automata_descriptor_path , execute_actions ) def consume_gesture ( self , static_gesture_buffer : GestureBuffer , dynamic_gesture_buffer : GestureBuffer ): super ( GestureController , self ) . _consume_gesture ( static_gesture_buffer , dynamic_gesture_buffer , input_accepted_callback = None ) It is the easiest implementation of the BaseController . The consume_gesture method invokes the _consume_gesture from the super class without any callback.","title":"Gesture controller"},{"location":"development/hand_gesture_recognizer/","text":"Hand gesture recognizer In the GestureDetector.py file there is the definition of the classes: GestureDetector : that uses MediaPipe and Tensorflow models to recognize hand gestures. GestureBuffer : that holds a sequence of gestures to use in successive steps. Initialization The method __init__(...) initialize all the variables needed for the recognizer to work: Load MediaPipe self . hands = mediapipe . solutions . hands . Hands ( static_image_mode = False , # We will use a real time image max_num_hands = 1 , # We will use only one hand min_detection_confidence = min_detection_confidence , min_tracking_confidence = min_tracking_confidence ) Load the static gesture classifier and the dynamic gesture classifier self . keypoint_classifier = KeyPointClassifier ( model_path = model_keypoint_classifier_path , num_threads = 1 ) with open ( labels_keypoint_classifier_path , encoding = 'utf-8-sig' ) as labels_file : self . keypoint_classifier_labels = [ row [ 0 ] for row in csv . reader ( labels_file )] self . point_history_classifier = PointHistoryClassifier ( model_path = model_point_history_classifier_path , num_threads = 1 , score_th = 0.5 , invalid_value = 0 ) with open ( labels_point_history_classifier_path , encoding = 'utf-8-sig' ) as labels_file : self . point_history_classifier_labels = [ row [ 0 ] for row in csv . reader ( labels_file )] Recognize The method recognize take as input: The image representing the frame to analyze The number representing the ID of the gesture in case the user want to save it ( learning mode ) The mode in which the program has been launched The fps to write on image to be more informative After the initialization of some variables used later the image is given as input to the MediaPipe model to receive in output the list of landmarks coordinate if a hand is founded in the image. If this happen, the landmarks coordinate are normalized and converted (to be compatible with the Tensorflow models) and given as input to the two classifier. The classifiers' output is written in the two buffers and the predicted gestures are returned. Static and dynamic gestures Each time the method is invoked it tries to recognize both a static gesture and a dynamic gesture and it returns both the predictions. # Bounding box calculation bounding_box_rect = calc_bounding_rect ( debug_image , hand_landmarks ) # Landmark calculation landmark_list = calc_landmark_list ( debug_image , hand_landmarks ) # Conversion to relative coordinates / normalized coordinates pre_processed_landmark_list = pre_process_landmark ( landmark_list ) pre_processed_point_history_list = pre_process_point_history ( debug_image , self . point_history ) # Write to dataset (1) logging_csv ( number , mode , pre_processed_landmark_list , pre_processed_point_history_list ) # Hand sign classification static_hand_gesture_id = self . keypoint_classifier ( pre_processed_landmark_list ) for i in [ 4 , 8 , 12 , 16 , 20 ]: # (2) self . point_history . append ( landmark_list [ i ]) # Finger gesture classification point_history_len = len ( pre_processed_point_history_list ) if point_history_len == ( self . history_length * 2 * 5 ): dynamic_hand_gesture_id = self . point_history_classifier ( pre_processed_point_history_list ) # Calculate the gesture IDs in the latest calculation self . finger_gesture_history . append ( dynamic_hand_gesture_id ) most_common_dynamic_gesture = Counter ( self . finger_gesture_history ) . most_common () # Drawing information on the image debug_image = draw_bounding_rect ( debug_image , bounding_box_rect ) debug_image = draw_landmarks ( debug_image , landmark_list ) debug_image = draw_info_text ( debug_image , bounding_box_rect , handedness , self . keypoint_classifier_labels [ static_hand_gesture_id ], self . point_history_classifier_labels [ most_common_dynamic_gesture [ 0 ][ 0 ]], ) It will write only if the mode is LEARING and the number is not None For the dynamic hand gestures the classifier uses the landmarks 4, 8, 12, 16, 20 that are the landmarks of the tip of the fingers.","title":"Hand gesture recognizer"},{"location":"development/hand_gesture_recognizer/#hand-gesture-recognizer","text":"In the GestureDetector.py file there is the definition of the classes: GestureDetector : that uses MediaPipe and Tensorflow models to recognize hand gestures. GestureBuffer : that holds a sequence of gestures to use in successive steps.","title":"Hand gesture recognizer"},{"location":"development/hand_gesture_recognizer/#initialization","text":"The method __init__(...) initialize all the variables needed for the recognizer to work: Load MediaPipe self . hands = mediapipe . solutions . hands . Hands ( static_image_mode = False , # We will use a real time image max_num_hands = 1 , # We will use only one hand min_detection_confidence = min_detection_confidence , min_tracking_confidence = min_tracking_confidence ) Load the static gesture classifier and the dynamic gesture classifier self . keypoint_classifier = KeyPointClassifier ( model_path = model_keypoint_classifier_path , num_threads = 1 ) with open ( labels_keypoint_classifier_path , encoding = 'utf-8-sig' ) as labels_file : self . keypoint_classifier_labels = [ row [ 0 ] for row in csv . reader ( labels_file )] self . point_history_classifier = PointHistoryClassifier ( model_path = model_point_history_classifier_path , num_threads = 1 , score_th = 0.5 , invalid_value = 0 ) with open ( labels_point_history_classifier_path , encoding = 'utf-8-sig' ) as labels_file : self . point_history_classifier_labels = [ row [ 0 ] for row in csv . reader ( labels_file )]","title":"Initialization"},{"location":"development/hand_gesture_recognizer/#recognize","text":"The method recognize take as input: The image representing the frame to analyze The number representing the ID of the gesture in case the user want to save it ( learning mode ) The mode in which the program has been launched The fps to write on image to be more informative After the initialization of some variables used later the image is given as input to the MediaPipe model to receive in output the list of landmarks coordinate if a hand is founded in the image. If this happen, the landmarks coordinate are normalized and converted (to be compatible with the Tensorflow models) and given as input to the two classifier. The classifiers' output is written in the two buffers and the predicted gestures are returned. Static and dynamic gestures Each time the method is invoked it tries to recognize both a static gesture and a dynamic gesture and it returns both the predictions. # Bounding box calculation bounding_box_rect = calc_bounding_rect ( debug_image , hand_landmarks ) # Landmark calculation landmark_list = calc_landmark_list ( debug_image , hand_landmarks ) # Conversion to relative coordinates / normalized coordinates pre_processed_landmark_list = pre_process_landmark ( landmark_list ) pre_processed_point_history_list = pre_process_point_history ( debug_image , self . point_history ) # Write to dataset (1) logging_csv ( number , mode , pre_processed_landmark_list , pre_processed_point_history_list ) # Hand sign classification static_hand_gesture_id = self . keypoint_classifier ( pre_processed_landmark_list ) for i in [ 4 , 8 , 12 , 16 , 20 ]: # (2) self . point_history . append ( landmark_list [ i ]) # Finger gesture classification point_history_len = len ( pre_processed_point_history_list ) if point_history_len == ( self . history_length * 2 * 5 ): dynamic_hand_gesture_id = self . point_history_classifier ( pre_processed_point_history_list ) # Calculate the gesture IDs in the latest calculation self . finger_gesture_history . append ( dynamic_hand_gesture_id ) most_common_dynamic_gesture = Counter ( self . finger_gesture_history ) . most_common () # Drawing information on the image debug_image = draw_bounding_rect ( debug_image , bounding_box_rect ) debug_image = draw_landmarks ( debug_image , landmark_list ) debug_image = draw_info_text ( debug_image , bounding_box_rect , handedness , self . keypoint_classifier_labels [ static_hand_gesture_id ], self . point_history_classifier_labels [ most_common_dynamic_gesture [ 0 ][ 0 ]], ) It will write only if the mode is LEARING and the number is not None For the dynamic hand gestures the classifier uses the landmarks 4, 8, 12, 16, 20 that are the landmarks of the tip of the fingers.","title":"Recognize"},{"location":"development/macro_controller/","text":"Macro controller When the operator wants to create a new macro using the gestures, this controller is used to write them on a text file instead of communicates with the robot. class MacroController ( BaseController ): def __init__ ( self , macro_file_path : str , keypoint_labels_path : str , point_history_labels_path : str , automata_descriptor_path : str , execute_actions : bool ): super () . __init__ ( keypoint_labels_path , point_history_labels_path , automata_descriptor_path , execute_actions ) self . macro_file_path = macro_file_path def consume_gesture ( self , static_gesture_buffer : GestureBuffer , dynamic_gesture_buffer : GestureBuffer ): super ( MacroController , self ) . _consume_gesture ( static_gesture_buffer , dynamic_gesture_buffer , input_accepted_callback = self . _write_on_file ) def _write_on_file ( self , input_gesture ): with open ( self . macro_file_path , \"a+\" ) as macro_file : macro_file . write ( f \" { input_gesture } \\n \" ) In particular the _write_on_file method is called as callback if the input is accepted by the automaton.","title":"MacroController"},{"location":"development/macro_controller/#macro-controller","text":"When the operator wants to create a new macro using the gestures, this controller is used to write them on a text file instead of communicates with the robot. class MacroController ( BaseController ): def __init__ ( self , macro_file_path : str , keypoint_labels_path : str , point_history_labels_path : str , automata_descriptor_path : str , execute_actions : bool ): super () . __init__ ( keypoint_labels_path , point_history_labels_path , automata_descriptor_path , execute_actions ) self . macro_file_path = macro_file_path def consume_gesture ( self , static_gesture_buffer : GestureBuffer , dynamic_gesture_buffer : GestureBuffer ): super ( MacroController , self ) . _consume_gesture ( static_gesture_buffer , dynamic_gesture_buffer , input_accepted_callback = self . _write_on_file ) def _write_on_file ( self , input_gesture ): with open ( self . macro_file_path , \"a+\" ) as macro_file : macro_file . write ( f \" { input_gesture } \\n \" ) In particular the _write_on_file method is called as callback if the input is accepted by the automaton.","title":"Macro controller"},{"location":"development/macro_runner/","text":"Macro runner This class does not inherit from the BaseController because it doesn't use buffers to retrieve the gestures. Instead, it reads a macro file and give the gestures written in it directly to the automaton. class MacroRunner : def __init__ ( self , macro_file_path : str , automata_descriptor_path : str ): self . state = 'q0' self . macro_file_path = macro_file_path self . automata = AutomataManager ( automata_descriptor_path , execute_actions = True ) def run ( self ): with open ( self . macro_file_path ) as macro_file : # actions = [item for sub_list in [el.split() for el in macro_file.readlines()] for item in sub_list] for line , action in enumerate ( macro_file . read () . splitlines ()): input_accepted = self . automata . consume_input ( action ) if not input_accepted : print ( f \"Macro file { self . macro_file_path } contains command { action } NOT ACCEPTED at line { line } \" ) sys . exit ( 1 )","title":"MacroRunner"},{"location":"development/macro_runner/#macro-runner","text":"This class does not inherit from the BaseController because it doesn't use buffers to retrieve the gestures. Instead, it reads a macro file and give the gestures written in it directly to the automaton. class MacroRunner : def __init__ ( self , macro_file_path : str , automata_descriptor_path : str ): self . state = 'q0' self . macro_file_path = macro_file_path self . automata = AutomataManager ( automata_descriptor_path , execute_actions = True ) def run ( self ): with open ( self . macro_file_path ) as macro_file : # actions = [item for sub_list in [el.split() for el in macro_file.readlines()] for item in sub_list] for line , action in enumerate ( macro_file . read () . splitlines ()): input_accepted = self . automata . consume_input ( action ) if not input_accepted : print ( f \"Macro file { self . macro_file_path } contains command { action } NOT ACCEPTED at line { line } \" ) sys . exit ( 1 )","title":"Macro runner"},{"location":"train/train_main/","text":"Train package The __main__.py file of the train package handles the CLI arguments, sets a seed to make tests reproducible, and invoke the right training process import argparse from tensorflow.python.framework.random_seed import set_seed from numpy.random import seed from .train_keypoint import train as train_static_gestures from .train_history import train as train_dynamic_gestures def cli_argument_parser () -> dict : arguments = argparse . ArgumentParser ( prog = \"Human-Robot in tandem Train\" ) arguments . add_argument ( \"train_model\" , choices = [ \"static_gesture\" , \"dynamic_gesture\" ], help = \"Choose which model train: static_gesture for the Keypoint Classifier, \\ dynamic_gesture for the Point History Classifier.\" ) arguments . add_argument ( \"--train-name\" , type = str ) arguments . add_argument ( \"--sample-number\" , type = int , required = False , help = \"The number of elements to pick for each class downsampling the dataset. \\ This operation is done before the train_test_split operation. \\ If a class hasn't enough samples then, the minimum is taken between all the other classes. \\ If this parameter is not set then, the whole dataset is used.\" ) return vars ( arguments . parse_args ()) if __name__ == \"__main__\" : set_seed ( 42 ) # Tensorflow seed ( 42 ) # NumPy args = cli_argument_parser () print ( args ) if args [ \"train_model\" ] == \"static_gesture\" : print ( \"Training on static hand gestures\" ) train_static_gestures ( ** args ) else : # args[\"train_model\"] == \"dynamic_gesture\" train_dynamic_gestures ( ** args ) print ( \"Training on dynamic hand gestures\" )","title":"Main"},{"location":"train/train_main/#train-package","text":"The __main__.py file of the train package handles the CLI arguments, sets a seed to make tests reproducible, and invoke the right training process import argparse from tensorflow.python.framework.random_seed import set_seed from numpy.random import seed from .train_keypoint import train as train_static_gestures from .train_history import train as train_dynamic_gestures def cli_argument_parser () -> dict : arguments = argparse . ArgumentParser ( prog = \"Human-Robot in tandem Train\" ) arguments . add_argument ( \"train_model\" , choices = [ \"static_gesture\" , \"dynamic_gesture\" ], help = \"Choose which model train: static_gesture for the Keypoint Classifier, \\ dynamic_gesture for the Point History Classifier.\" ) arguments . add_argument ( \"--train-name\" , type = str ) arguments . add_argument ( \"--sample-number\" , type = int , required = False , help = \"The number of elements to pick for each class downsampling the dataset. \\ This operation is done before the train_test_split operation. \\ If a class hasn't enough samples then, the minimum is taken between all the other classes. \\ If this parameter is not set then, the whole dataset is used.\" ) return vars ( arguments . parse_args ()) if __name__ == \"__main__\" : set_seed ( 42 ) # Tensorflow seed ( 42 ) # NumPy args = cli_argument_parser () print ( args ) if args [ \"train_model\" ] == \"static_gesture\" : print ( \"Training on static hand gestures\" ) train_static_gestures ( ** args ) else : # args[\"train_model\"] == \"dynamic_gesture\" train_dynamic_gestures ( ** args ) print ( \"Training on dynamic hand gestures\" )","title":"Train package"},{"location":"train/train_networks/","text":"Train static and dynamic hand gesture The process to train the two networks is similar. It only differs in the topology of the network. The model is built with the function get_model . The train function calls some other functions to read the dataset in the right way: get_num_classes_from_labels_file(labels_path) get_dataset(dataset_path, sample_number) get_train_test_evaluate_datasets(x_dataset, y_dataset, train_size=0.75, evaluation_size=0.2) Then, with two functions that take also the time the model is trained and evaluated: train_model_with_time(model, x_train, y_train, x_test, y_test, callbacks=get_callbacks(model_save_path, log_dir), batch_size=64, epochs=10000) evaluate_model_with_time(model, x_evaluate, y_evaluate, batch_size=64) Finally, the model is converted to tflite with another function: convert_to_tflite(model, tflite_save_path) Get the dataset Get the number of classes from labels file def get_num_classes_from_labels_file ( file_path : str ) -> int : with open ( file_path , \"r\" ) as labels_file : labels = [ label . replace ( \" \\n \" , \"\" ) . replace ( '\"' , '' ) for label in labels_file . readlines ()] print ( \"Labels: \" , labels ) num_classes = len ( labels ) return num_classes Read the dataset This function read the dataset but, if a value is passed as sample_number , then only a random subset with sample_number elements for each class is taken def get_dataset ( dataset_path : str , sample_number : [ int | None ]) -> tuple [ ndarray , ndarray ]: df = read_csv ( dataset_path , header = None ) classes = df [ 0 ] . unique () # Downsampling if sample_number : print ( \"Undersampling...\" ) reduced_dfs = [ df [ df [ 0 ] == i ] . sample ( sample_number , random_state = 42 ) for i in classes ] df = concat ( reduced_dfs ) . sample ( frac = 1 ) . reset_index ( drop = True ) return df [ 0 ] . to_numpy ( dtype = 'int32' ), df . drop ( df . columns [[ 0 ]], axis = 1 ) . to_numpy ( dtype = 'float32' ) Split the dataset Uses the train_test_split function of sklearn to randomly split the dataset def get_train_test_evaluate_datasets ( x_dataset : ndarray , y_dataset : ndarray , train_size : float , evaluation_size : float ) -> tuple [ ndarray , ndarray , ndarray , ndarray , ndarray , ndarray ]: x_train , x_test , y_train , y_test = train_test_split ( x_dataset , y_dataset , train_size = train_size ) x_train , x_evaluate , y_train , y_evaluate = train_test_split ( x_train , y_train , train_size = 1 - evaluation_size ) return x_train , y_train , x_test , y_test , x_evaluate , y_evaluate Train and evaluate with time A decorator is used to take time of the fit and evaluate functions def take_time ( name : str ): def decorator ( function ): def inner ( * args , ** kwargs ): start = datetime . now () function_result = function ( * args , ** kwargs ) end = datetime . now () print ( f \"Function { name } took { end - start } to execute\" ) return function_result return inner return decorator @take_time ( \"Keypoint fit\" ) def train_model_with_time ( model , x_train , y_train , x_test , y_test , callbacks , batch_size , epochs ): # Model train model . fit ( x_train , y_train , epochs = epochs , batch_size = batch_size , validation_data = ( x_test , y_test ), callbacks = callbacks ) @take_time ( \"Keypoint evaluate\" ) def evaluate_model_with_time ( model , x_evaluate , y_evaluate , batch_size ): # Model evaluation val_loss , val_acc = model . evaluate ( x_evaluate , y_evaluate , batch_size = batch_size ) print ( \"Validation loss:\" , val_loss , \" \\n Validation accuracy:\" , val_acc ) Callbacks The callbacks are got with the function get_callbacks Convert to tflite def convert_to_tflite ( model : str , tflite_file_path ): # Conversion to tflite converter = tf . lite . TFLiteConverter . from_keras_model ( model ) converter . optimizations = [ tf . lite . Optimize . DEFAULT ] tflite_quantized_model = converter . convert () with open ( tflite_file_path , 'wb' ) as tflite_file : tflite_file . write ( tflite_quantized_model )","title":"Train static and dynamic hand gesture"},{"location":"train/train_networks/#train-static-and-dynamic-hand-gesture","text":"The process to train the two networks is similar. It only differs in the topology of the network. The model is built with the function get_model . The train function calls some other functions to read the dataset in the right way: get_num_classes_from_labels_file(labels_path) get_dataset(dataset_path, sample_number) get_train_test_evaluate_datasets(x_dataset, y_dataset, train_size=0.75, evaluation_size=0.2) Then, with two functions that take also the time the model is trained and evaluated: train_model_with_time(model, x_train, y_train, x_test, y_test, callbacks=get_callbacks(model_save_path, log_dir), batch_size=64, epochs=10000) evaluate_model_with_time(model, x_evaluate, y_evaluate, batch_size=64) Finally, the model is converted to tflite with another function: convert_to_tflite(model, tflite_save_path)","title":"Train static and dynamic hand gesture"},{"location":"train/train_networks/#get-the-dataset","text":"","title":"Get the dataset"},{"location":"train/train_networks/#get-the-number-of-classes-from-labels-file","text":"def get_num_classes_from_labels_file ( file_path : str ) -> int : with open ( file_path , \"r\" ) as labels_file : labels = [ label . replace ( \" \\n \" , \"\" ) . replace ( '\"' , '' ) for label in labels_file . readlines ()] print ( \"Labels: \" , labels ) num_classes = len ( labels ) return num_classes","title":"Get the number of classes from labels file"},{"location":"train/train_networks/#read-the-dataset","text":"This function read the dataset but, if a value is passed as sample_number , then only a random subset with sample_number elements for each class is taken def get_dataset ( dataset_path : str , sample_number : [ int | None ]) -> tuple [ ndarray , ndarray ]: df = read_csv ( dataset_path , header = None ) classes = df [ 0 ] . unique () # Downsampling if sample_number : print ( \"Undersampling...\" ) reduced_dfs = [ df [ df [ 0 ] == i ] . sample ( sample_number , random_state = 42 ) for i in classes ] df = concat ( reduced_dfs ) . sample ( frac = 1 ) . reset_index ( drop = True ) return df [ 0 ] . to_numpy ( dtype = 'int32' ), df . drop ( df . columns [[ 0 ]], axis = 1 ) . to_numpy ( dtype = 'float32' )","title":"Read the dataset"},{"location":"train/train_networks/#split-the-dataset","text":"Uses the train_test_split function of sklearn to randomly split the dataset def get_train_test_evaluate_datasets ( x_dataset : ndarray , y_dataset : ndarray , train_size : float , evaluation_size : float ) -> tuple [ ndarray , ndarray , ndarray , ndarray , ndarray , ndarray ]: x_train , x_test , y_train , y_test = train_test_split ( x_dataset , y_dataset , train_size = train_size ) x_train , x_evaluate , y_train , y_evaluate = train_test_split ( x_train , y_train , train_size = 1 - evaluation_size ) return x_train , y_train , x_test , y_test , x_evaluate , y_evaluate","title":"Split the dataset"},{"location":"train/train_networks/#train-and-evaluate-with-time","text":"A decorator is used to take time of the fit and evaluate functions def take_time ( name : str ): def decorator ( function ): def inner ( * args , ** kwargs ): start = datetime . now () function_result = function ( * args , ** kwargs ) end = datetime . now () print ( f \"Function { name } took { end - start } to execute\" ) return function_result return inner return decorator @take_time ( \"Keypoint fit\" ) def train_model_with_time ( model , x_train , y_train , x_test , y_test , callbacks , batch_size , epochs ): # Model train model . fit ( x_train , y_train , epochs = epochs , batch_size = batch_size , validation_data = ( x_test , y_test ), callbacks = callbacks ) @take_time ( \"Keypoint evaluate\" ) def evaluate_model_with_time ( model , x_evaluate , y_evaluate , batch_size ): # Model evaluation val_loss , val_acc = model . evaluate ( x_evaluate , y_evaluate , batch_size = batch_size ) print ( \"Validation loss:\" , val_loss , \" \\n Validation accuracy:\" , val_acc )","title":"Train and evaluate with time"},{"location":"train/train_networks/#callbacks","text":"The callbacks are got with the function get_callbacks","title":"Callbacks"},{"location":"train/train_networks/#convert-to-tflite","text":"def convert_to_tflite ( model : str , tflite_file_path ): # Conversion to tflite converter = tf . lite . TFLiteConverter . from_keras_model ( model ) converter . optimizations = [ tf . lite . Optimize . DEFAULT ] tflite_quantized_model = converter . convert () with open ( tflite_file_path , 'wb' ) as tflite_file : tflite_file . write ( tflite_quantized_model )","title":"Convert to tflite"}]}